{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "I loaded in the files for each search location, extracted the dictionaries and turned them into columns, and concatenated them all to build one dataset per search location. In the end, I have three dataframes. One for DC, one for US, and one with both. I also added a column for the search location to know which data came from where.\n",
    "\n",
    "This code for the DC search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the folder containing data files\n",
    "folder_path = '../Data/DC_Search/'\n",
    "\n",
    "# create an empty list to store the dataframes\n",
    "dfs = []\n",
    "\n",
    "# loop over all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # check if the file is a JSON file\n",
    "    if filename.endswith('.json'):\n",
    "        # read JSON file\n",
    "        with open(os.path.join(folder_path, filename)) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # extract the \"jobs_results\" dictionary\n",
    "        jobs_results = data['jobs_results']\n",
    "\n",
    "        # convert the dictionary into a DataFrame\n",
    "        df = pd.DataFrame.from_dict(jobs_results)\n",
    "\n",
    "        # append the dataframe to the list of dataframes\n",
    "        dfs.append(df)\n",
    "\n",
    "# concatenate all the dataframes into one\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "### extracting dictionaries from detected_extensions column\n",
    "# Assuming your DataFrame is named \"df\"\n",
    "def extract_dict_columns(d):\n",
    "    \"\"\"Custom function to extract dictionary keys into separate columns\"\"\"\n",
    "    keys = [\"schedule_type\", \"salary\", \"posted_at\", \"work_from_home\"]\n",
    "    return {\n",
    "        \"schedule_type\": d.get(\"schedule_type\", np.nan),\n",
    "        \"salary\": d.get(\"salary\", np.nan),\n",
    "        \"posted_at\": d.get(\"posted_at\", np.nan),\n",
    "        \"work_from_home\": d.get(\"work_from_home\", np.nan)\n",
    "    }\n",
    "\n",
    "# Use apply method with the custom function to extract the dictionary keys into separate columns\n",
    "final_df[[\"schedule_type\", \"salary\", \"posted_at\", \"work_from_home\"]] = final_df[\"detected_extensions\"].apply(lambda x: pd.Series(extract_dict_columns(x)))\n",
    "\n",
    "# Drop the original column containing the nested dictionaries\n",
    "final_df = final_df.drop([\"detected_extensions\", \"job_id\", \"related_links\", \"extensions\"], axis=1)\n",
    "\n",
    "# get qualifications, responsibilities, and benefits\n",
    "quals = pd.json_normalize(pd.json_normalize(final_df['job_highlights'])[0])['items']\n",
    "resps = pd.json_normalize(pd.json_normalize(final_df['job_highlights'])[1])['items']\n",
    "bens = pd.json_normalize(pd.json_normalize(final_df['job_highlights'])[2])['items']\n",
    "\n",
    "# add to dataframe as columns\n",
    "final_df['Qualifications'] = quals\n",
    "final_df['Responsibilities'] = resps\n",
    "final_df['Benefits'] = bens\n",
    "\n",
    "# drop job_highlights column\n",
    "final_df = final_df.drop(['job_highlights'], axis=1)\n",
    "\n",
    "# add a column for the search location\n",
    "final_df['Search_Location'] = 'DC'\n",
    "\n",
    "# rename it to DC_search\n",
    "DC_search = final_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same process for the US search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the folder containing data files\n",
    "folder_path = '../Data/USA_Search/'\n",
    "\n",
    "# create an empty list to store the dataframes\n",
    "dfs = []\n",
    "\n",
    "# loop over all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # check if the file is a JSON file\n",
    "    if filename.endswith('.json'):\n",
    "        # read JSON file\n",
    "        with open(os.path.join(folder_path, filename)) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # extract the \"jobs_results\" dictionary\n",
    "        jobs_results = data['jobs_results']\n",
    "\n",
    "        # convert the dictionary into a DataFrame\n",
    "        df = pd.DataFrame.from_dict(jobs_results)\n",
    "\n",
    "        # append the dataframe to the list of dataframes\n",
    "        dfs.append(df)\n",
    "\n",
    "# concatenate all the dataframes into one\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "### extracting dictionaries from detected_extensions column\n",
    "# Assuming your DataFrame is named \"df\"\n",
    "def extract_dict_columns(d):\n",
    "    \"\"\"Custom function to extract dictionary keys into separate columns\"\"\"\n",
    "    keys = [\"schedule_type\", \"salary\", \"posted_at\", \"work_from_home\"]\n",
    "    return {\n",
    "        \"schedule_type\": d.get(\"schedule_type\", np.nan),\n",
    "        \"salary\": d.get(\"salary\", np.nan),\n",
    "        \"posted_at\": d.get(\"posted_at\", np.nan),\n",
    "        \"work_from_home\": d.get(\"work_from_home\", np.nan)\n",
    "    }\n",
    "\n",
    "# Use apply method with the custom function to extract the dictionary keys into separate columns\n",
    "final_df[[\"schedule_type\", \"salary\", \"posted_at\", \"work_from_home\"]] = final_df[\"detected_extensions\"].apply(lambda x: pd.Series(extract_dict_columns(x)))\n",
    "\n",
    "# Drop the original column containing the nested dictionaries\n",
    "final_df = final_df.drop([\"detected_extensions\", \"job_id\", \"related_links\", \"extensions\"], axis=1)\n",
    "\n",
    "# get qualifications, responsibilities, and benefits\n",
    "quals = pd.json_normalize(pd.json_normalize(final_df['job_highlights'])[0])['items']\n",
    "resps = pd.json_normalize(pd.json_normalize(final_df['job_highlights'])[1])['items']\n",
    "bens = pd.json_normalize(pd.json_normalize(final_df['job_highlights'])[2])['items']\n",
    "\n",
    "# add to dataframe as columns\n",
    "final_df['Qualifications'] = quals\n",
    "final_df['Responsibilities'] = resps\n",
    "final_df['Benefits'] = bens\n",
    "\n",
    "# drop job_highlights column\n",
    "final_df = final_df.drop(['job_highlights'], axis=1)\n",
    "\n",
    "# add a column for the search location\n",
    "final_df['Search_Location'] = 'USA'\n",
    "\n",
    "# rename it to DC_search\n",
    "USA_search = final_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two dataframes\n",
    "all = pd.concat([DC_search, USA_search], ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing duplicates\n",
    "- Strip leading and trailing whitespace from posted_at column\n",
    "- Clean posted_at column\n",
    "- Cleaning via column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows\n",
    "all = all.drop_duplicates(subset=['description'], keep='first')\n",
    "\n",
    "# remove \"ago\" from posted_at column\n",
    "all['posted_at'] = all['posted_at'].str.replace('ago', '')\n",
    "\n",
    "# strip leading or trailing whitespace from posted_at column\n",
    "all['posted_at'] = all['posted_at'].str.strip()\n",
    "\n",
    "# change NaNs to not available\n",
    "all['posted_at'] = all['posted_at'].fillna('Not Available')\n",
    "\n",
    "# change anything that has hours to 1 day\n",
    "all['posted_at'] = all['posted_at'].apply(lambda x: '1 day' if 'hour' in x else x)\n",
    "\n",
    "# remove the one row that has month in posted_at column\n",
    "all = all[all['posted_at'] != '1 month']\n",
    "\n",
    "# change \"not available\" to NaN\n",
    "all['posted_at'] = all['posted_at'].replace('Not Available', np.nan)\n",
    "\n",
    "# remove days and day from posted_at column and keep only the number\n",
    "all['posted_at'] = all['posted_at'].str.replace(' days', '')\n",
    "all['posted_at'] = all['posted_at'].str.replace(' day', '')\n",
    "\n",
    "# change posted_at column name to days_posted\n",
    "all = all.rename(columns={'posted_at': 'days_posted'})\n",
    "\n",
    "# removing \"via\" from via column\n",
    "all['via'] = all['via'].str.replace('via ', '')\n",
    "\n",
    "# remove trailing and leading whitespace from via column\n",
    "all['via'] = all['via'].str.strip()\n",
    "\n",
    "# remove leading and trailing whitespace from all text columns\n",
    "all['title'] = all['title'].str.strip()\n",
    "all['company_name'] = all['company_name'].str.strip()\n",
    "all['location'] = all['location'].str.strip()\n",
    "all['description'] = all['description'].str.strip()\n",
    "all['schedule_type'] = all['schedule_type'].str.strip()\n",
    "all['salary'] = all['salary'].str.strip()\n",
    "\n",
    "# add salary ranges columns\n",
    "# change benefit NaNs to a list with \"Not Available\"\n",
    "all['Benefits'] = all['Benefits'].fillna('Not Available')\n",
    "salaries = []\n",
    "\n",
    "for x in all['Benefits']:\n",
    "    if x == 'Not Available':\n",
    "        salaries.append([np.nan])\n",
    "    elif re.findall(r'\\$\\d+(?:,\\d+)*(?:\\.\\d+)?', ''.join(x)):\n",
    "        salaries.append(re.findall(r'\\$\\d+(?:,\\d+)*(?:\\.\\d+)?', ''.join(x)))\n",
    "    else:\n",
    "        salary_found = False\n",
    "        for y in x:\n",
    "            if re.findall(r'\\$\\d+(?:,\\d+)*(?:\\.\\d+)?', y) == []:\n",
    "                continue\n",
    "            else:\n",
    "                salaries.append(re.findall(r'\\$\\d+(?:,\\d+)*(?:\\.\\d+)?', y))\n",
    "                salary_found = True\n",
    "        if not salary_found:\n",
    "            salaries.append([np.nan])\n",
    "\n",
    "# return the first and last value of each list within salaries\n",
    "min_salary = [x[0] for x in salaries]\n",
    "max_salary = [x[-1] for x in salaries]\n",
    "\n",
    "# add min and max salary columns to dataframe\n",
    "all['min_salary'] = min_salary\n",
    "all['max_salary'] = max_salary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on an average work hours per year of 2,087, I calculated the yearly salary for each job posting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/37t7tc411k5_r_7l10g6cy6r0000gn/T/ipykernel_38773/3256195869.py:2: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  sals = all['salary'].str.split(' ', 1).str[0]\n"
     ]
    }
   ],
   "source": [
    "# remove unnecessary text from salary column\n",
    "sals = all['salary'].str.split(' ', 1).str[0]\n",
    "\n",
    "# replace NaNs with 0-0\n",
    "sals = sals.fillna('0-0')\n",
    "\n",
    "# split the salary column into min and max salary columns\n",
    "myl = []\n",
    "for value in sals:\n",
    "    split_values = re.split(\"-|â€“\", value)\n",
    "    myl.append(split_values)\n",
    "\n",
    "# split myl into two lists\n",
    "min_sal = [x[0] for x in myl]\n",
    "max_sal = [x[-1] for x in myl]\n",
    "\n",
    "# add min and max salary columns to dataframe\n",
    "all['min_sal'] = min_sal\n",
    "all['max_sal'] = max_sal\n",
    "\n",
    "# if min_sal or max_sal has a K, replace it with 000\n",
    "all['min_sal'] = all['min_sal'].str.replace('K', '000')\n",
    "all['max_sal'] = all['max_sal'].str.replace('K', '000')\n",
    "\n",
    "# remove commas from min_sal and max_sal columns\n",
    "all['min_sal'] = all['min_sal'].str.replace(',', '')\n",
    "all['max_sal'] = all['max_sal'].str.replace(',', '')\n",
    "\n",
    "# change min_sal and max_sal columns to numeric\n",
    "all['min_sal'] = pd.to_numeric(all['min_sal'])\n",
    "all['max_sal'] = pd.to_numeric(all['max_sal'])\n",
    "\n",
    "# if min_sal or max_sal is less than 300, multiply by 2087\n",
    "all['min_sal'] = all['min_sal'].apply(lambda x: x*2087 if x < 300 else x)\n",
    "all['max_sal'] = all['max_sal'].apply(lambda x: x*2087 if x < 300 else x)\n",
    "\n",
    "# if min_salary, max_salary are empty, fill them with min_sal, max_sal\n",
    "all['min_salary'] = all['min_salary'].fillna(all['min_sal'])\n",
    "all['max_salary'] = all['max_salary'].fillna(all['max_sal'])\n",
    "\n",
    "# remove min_sal and max_sal columns\n",
    "all = all.drop(['min_sal', 'max_sal'], axis=1)\n",
    "\n",
    "# remove $ and , from min_salary and max_salary columns and turn them into floats\n",
    "all['min_salary'] = all['min_salary'].apply(lambda x: str(x).replace(',', '').replace('$', '') if isinstance(x, str) else x).astype(float)\n",
    "all['max_salary'] = all['max_salary'].apply(lambda x: str(x).replace(',', '').replace('$', '') if isinstance(x, str) else x).astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the rows with errors to manually fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# index 15\n",
    "all.loc[15, 'max_salary'] = 245700\n",
    "\n",
    "# index 94\n",
    "all.loc[94, 'max_salary'] = 100000\n",
    "all.loc[94, 'min_salary'] = 80000\n",
    "\n",
    "# index 105\n",
    "all.loc[105, 'max_salary'] = 370000\n",
    "all.loc[105, 'min_salary'] = 200000\n",
    "\n",
    "# index 218\n",
    "all.loc[218, 'max_salary'] = 370000\n",
    "all.loc[218, 'min_salary'] = 200000\n",
    "\n",
    "# index 224\n",
    "all.loc[224, 'max_salary'] = 335000\n",
    "all.loc[224, 'min_salary'] = 230000\n",
    "\n",
    "# index 258\n",
    "all.loc[258, 'max_salary'] = 100000\n",
    "all.loc[258, 'min_salary'] = 80000\n",
    "\n",
    "# index 344\n",
    "all.loc[344, 'max_salary'] = 335000\n",
    "all.loc[344, 'min_salary'] = 196000\n",
    "\n",
    "# index 481\n",
    "all.loc[481, 'max_salary'] = 28*2087\n",
    "all.loc[481, 'min_salary'] = 20*2087\n",
    "\n",
    "# index 582\n",
    "all.loc[582, 'max_salary'] = 100000\n",
    "all.loc[582, 'min_salary'] = 80000\n",
    "\n",
    "# index 739\n",
    "all.loc[739, 'max_salary'] = 250128\n",
    "all.loc[739, 'min_salary'] = 168070\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove salary column\n",
    "all = all.drop(['salary'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for range between min and max salary\n",
    "all['salary_range'] = all['max_salary'] - all['min_salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          Not Available\n",
       "1      [(NYC only) Pursuant to Section 8-102 of title...\n",
       "2                                          Not Available\n",
       "3                                          Not Available\n",
       "4                                          Not Available\n",
       "                             ...                        \n",
       "806                                        Not Available\n",
       "807    [US salary range: $150,000 - $250,000, We offe...\n",
       "812                                        Not Available\n",
       "815                                        Not Available\n",
       "820    [Besides a great work environment, our compreh...\n",
       "Name: Benefits, Length: 578, dtype: object"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all['Benefits']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
